{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1732825,"sourceType":"datasetVersion","datasetId":1028436},{"sourceId":6676302,"sourceType":"datasetVersion","datasetId":3851864}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\n\n# Function to load and preprocess images\ndef load_and_preprocess_data(directory):\n    images = []\n    labels = []\n    emotion_labels = os.listdir(directory)\n\n    for label, emotion in enumerate(emotion_labels):\n        emotion_dir = os.path.join(directory, emotion)\n        for img_name in os.listdir(emotion_dir):\n            img_path = os.path.join(emotion_dir, img_name)\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n            img = cv2.resize(img, (48, 48))  # Resize images to a common size\n            images.append(img)  # Do not flatten the image for RNN\n            labels.append(label)\n\n    return np.array(images), np.array(labels)\n\n# Load and preprocess the data\nroot_dir = '/kaggle/input/emotion-detection-fer/'\ntrain_dir = os.path.join(root_dir, 'train')\ntest_dir = os.path.join(root_dir, 'test')\n\nX_train, y_train = load_and_preprocess_data(train_dir)\nX_test, y_test = load_and_preprocess_data(test_dir)\n\n# Normalize pixel values to be between 0 and 1\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\n# One-hot encode the labels\nnum_classes = len(os.listdir(train_dir))\ny_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)\n\n# Reshape data for LSTM\nX_train_lstm = X_train.reshape(X_train.shape[0], 48, 48)\nX_test_lstm = X_test.reshape(X_test.shape[0], 48, 48)\n\n# Split the training data into training and validation sets\nX_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(X_train_lstm, y_train, test_size=0.2, random_state=42)\n\n# Define the LSTM model\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(256, input_shape=(48, 48), return_sequences=True))\nmodel_lstm.add(Dropout(0.5))\nmodel_lstm.add(LSTM(128))\nmodel_lstm.add(Dropout(0.5))\nmodel_lstm.add(Dense(num_classes, activation='softmax'))\n\n# Compile the LSTM model\nmodel_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the LSTM model\nhistory_lstm = model_lstm.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32, validation_data=(X_val_lstm, y_val_lstm))\n\n# Evaluate the LSTM model on the test set\ntest_loss_lstm, test_accuracy_lstm = model_lstm.evaluate(X_test_lstm, y_test)\nprint(f'Test Accuracy (LSTM): {test_accuracy_lstm}')\n\n# Visualize LSTM training history\nplt.plot(history_lstm.history['accuracy'], label='Training Accuracy (LSTM)')\nplt.plot(history_lstm.history['val_accuracy'], label='Validation Accuracy (LSTM)')\nplt.title('Training and Validation Accuracy (LSTM)')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T07:18:08.813192Z","iopub.execute_input":"2024-01-02T07:18:08.813657Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n718/718 [==============================] - 122s 164ms/step - loss: 1.7742 - accuracy: 0.2770 - val_loss: 1.7256 - val_accuracy: 0.3060\nEpoch 2/100\n718/718 [==============================] - 117s 163ms/step - loss: 1.7065 - accuracy: 0.3202 - val_loss: 1.6846 - val_accuracy: 0.3426\nEpoch 3/100\n718/718 [==============================] - 117s 163ms/step - loss: 1.6708 - accuracy: 0.3399 - val_loss: 1.6704 - val_accuracy: 0.3405\nEpoch 4/100\n718/718 [==============================] - 132s 184ms/step - loss: 1.6290 - accuracy: 0.3630 - val_loss: 1.6005 - val_accuracy: 0.3687\nEpoch 5/100\n718/718 [==============================] - 138s 192ms/step - loss: 1.5930 - accuracy: 0.3788 - val_loss: 1.5734 - val_accuracy: 0.3819\nEpoch 6/100\n718/718 [==============================] - 141s 196ms/step - loss: 1.5707 - accuracy: 0.3866 - val_loss: 1.5896 - val_accuracy: 0.3713\nEpoch 7/100\n718/718 [==============================] - 139s 193ms/step - loss: 1.5522 - accuracy: 0.3956 - val_loss: 1.5520 - val_accuracy: 0.3960\nEpoch 8/100\n718/718 [==============================] - 153s 213ms/step - loss: 1.5215 - accuracy: 0.4089 - val_loss: 1.5222 - val_accuracy: 0.4033\nEpoch 9/100\n718/718 [==============================] - 153s 213ms/step - loss: 1.4953 - accuracy: 0.4181 - val_loss: 1.5039 - val_accuracy: 0.4114\nEpoch 10/100\n718/718 [==============================] - 153s 213ms/step - loss: 1.4749 - accuracy: 0.4298 - val_loss: 1.4934 - val_accuracy: 0.4228\nEpoch 11/100\n718/718 [==============================] - 153s 213ms/step - loss: 1.4465 - accuracy: 0.4409 - val_loss: 1.4839 - val_accuracy: 0.4270\nEpoch 12/100\n718/718 [==============================] - 143s 199ms/step - loss: 1.4190 - accuracy: 0.4494 - val_loss: 1.4698 - val_accuracy: 0.4335\nEpoch 13/100\n718/718 [==============================] - 153s 213ms/step - loss: 1.3936 - accuracy: 0.4604 - val_loss: 1.4667 - val_accuracy: 0.4324\nEpoch 14/100\n718/718 [==============================] - 153s 213ms/step - loss: 1.3686 - accuracy: 0.4746 - val_loss: 1.4601 - val_accuracy: 0.4422\nEpoch 15/100\n718/718 [==============================] - 152s 212ms/step - loss: 1.3387 - accuracy: 0.4843 - val_loss: 1.4752 - val_accuracy: 0.4420\nEpoch 16/100\n718/718 [==============================] - 155s 215ms/step - loss: 1.3113 - accuracy: 0.4953 - val_loss: 1.4456 - val_accuracy: 0.4472\nEpoch 17/100\n718/718 [==============================] - 155s 215ms/step - loss: 1.2807 - accuracy: 0.5102 - val_loss: 1.4549 - val_accuracy: 0.4474\nEpoch 18/100\n718/718 [==============================] - 142s 198ms/step - loss: 1.2559 - accuracy: 0.5193 - val_loss: 1.4428 - val_accuracy: 0.4554\nEpoch 19/100\n718/718 [==============================] - 132s 184ms/step - loss: 1.2212 - accuracy: 0.5433 - val_loss: 1.4465 - val_accuracy: 0.4573\nEpoch 20/100\n718/718 [==============================] - 132s 184ms/step - loss: 1.2001 - accuracy: 0.5454 - val_loss: 1.4759 - val_accuracy: 0.4544\nEpoch 21/100\n718/718 [==============================] - 145s 201ms/step - loss: 1.1675 - accuracy: 0.5578 - val_loss: 1.4826 - val_accuracy: 0.4521\nEpoch 22/100\n718/718 [==============================] - 162s 225ms/step - loss: 1.1350 - accuracy: 0.5733 - val_loss: 1.5047 - val_accuracy: 0.4580\nEpoch 23/100\n718/718 [==============================] - 155s 216ms/step - loss: 1.1155 - accuracy: 0.5821 - val_loss: 1.4735 - val_accuracy: 0.4664\nEpoch 24/100\n718/718 [==============================] - 154s 215ms/step - loss: 1.0786 - accuracy: 0.5957 - val_loss: 1.5549 - val_accuracy: 0.4525\nEpoch 25/100\n718/718 [==============================] - 147s 205ms/step - loss: 1.0607 - accuracy: 0.6050 - val_loss: 1.5448 - val_accuracy: 0.4667\nEpoch 26/100\n718/718 [==============================] - 147s 204ms/step - loss: 1.0243 - accuracy: 0.6189 - val_loss: 1.5351 - val_accuracy: 0.4633\nEpoch 27/100\n718/718 [==============================] - 154s 215ms/step - loss: 1.0034 - accuracy: 0.6244 - val_loss: 1.6066 - val_accuracy: 0.4709\nEpoch 28/100\n718/718 [==============================] - 158s 220ms/step - loss: 0.9749 - accuracy: 0.6396 - val_loss: 1.5520 - val_accuracy: 0.4753\nEpoch 29/100\n718/718 [==============================] - 157s 218ms/step - loss: 0.9492 - accuracy: 0.6472 - val_loss: 1.6196 - val_accuracy: 0.4697\nEpoch 30/100\n718/718 [==============================] - 157s 218ms/step - loss: 0.9279 - accuracy: 0.6548 - val_loss: 1.5803 - val_accuracy: 0.4638\nEpoch 31/100\n718/718 [==============================] - 154s 215ms/step - loss: 0.9102 - accuracy: 0.6662 - val_loss: 1.6402 - val_accuracy: 0.4678\nEpoch 32/100\n718/718 [==============================] - 161s 224ms/step - loss: 0.8859 - accuracy: 0.6743 - val_loss: 1.6715 - val_accuracy: 0.4744\nEpoch 33/100\n718/718 [==============================] - 155s 215ms/step - loss: 0.8645 - accuracy: 0.6814 - val_loss: 1.6774 - val_accuracy: 0.4692\nEpoch 34/100\n718/718 [==============================] - 153s 214ms/step - loss: 0.8532 - accuracy: 0.6904 - val_loss: 1.6659 - val_accuracy: 0.4739\nEpoch 35/100\n718/718 [==============================] - 154s 214ms/step - loss: 0.8204 - accuracy: 0.6986 - val_loss: 1.7408 - val_accuracy: 0.4671\nEpoch 36/100\n718/718 [==============================] - 154s 214ms/step - loss: 0.8034 - accuracy: 0.7058 - val_loss: 1.7043 - val_accuracy: 0.4608\nEpoch 37/100\n718/718 [==============================] - 149s 208ms/step - loss: 0.7869 - accuracy: 0.7152 - val_loss: 1.8223 - val_accuracy: 0.4617\nEpoch 38/100\n718/718 [==============================] - 138s 193ms/step - loss: 0.7610 - accuracy: 0.7235 - val_loss: 1.7917 - val_accuracy: 0.4594\nEpoch 39/100\n718/718 [==============================] - 128s 178ms/step - loss: 0.7457 - accuracy: 0.7266 - val_loss: 1.7888 - val_accuracy: 0.4627\nEpoch 40/100\n718/718 [==============================] - 138s 192ms/step - loss: 0.7448 - accuracy: 0.7322 - val_loss: 1.7760 - val_accuracy: 0.4674\nEpoch 41/100\n718/718 [==============================] - 143s 199ms/step - loss: 0.7149 - accuracy: 0.7449 - val_loss: 1.8595 - val_accuracy: 0.4631\nEpoch 42/100\n718/718 [==============================] - 143s 199ms/step - loss: 0.6992 - accuracy: 0.7492 - val_loss: 1.8480 - val_accuracy: 0.4683\nEpoch 43/100\n718/718 [==============================] - 148s 206ms/step - loss: 0.6838 - accuracy: 0.7543 - val_loss: 1.9447 - val_accuracy: 0.4619\nEpoch 44/100\n718/718 [==============================] - 152s 212ms/step - loss: 0.6757 - accuracy: 0.7586 - val_loss: 1.9016 - val_accuracy: 0.4633\nEpoch 45/100\n718/718 [==============================] - 146s 204ms/step - loss: 0.6618 - accuracy: 0.7637 - val_loss: 1.9301 - val_accuracy: 0.4735\nEpoch 46/100\n718/718 [==============================] - 138s 193ms/step - loss: 0.6515 - accuracy: 0.7655 - val_loss: 1.9204 - val_accuracy: 0.4619\nEpoch 47/100\n718/718 [==============================] - 146s 204ms/step - loss: 0.6253 - accuracy: 0.7760 - val_loss: 1.9777 - val_accuracy: 0.4667\nEpoch 48/100\n718/718 [==============================] - 153s 213ms/step - loss: 0.6167 - accuracy: 0.7817 - val_loss: 2.0242 - val_accuracy: 0.4690\nEpoch 49/100\n718/718 [==============================] - 152s 212ms/step - loss: 0.6065 - accuracy: 0.7843 - val_loss: 2.0097 - val_accuracy: 0.4671\nEpoch 50/100\n718/718 [==============================] - 149s 208ms/step - loss: 0.5994 - accuracy: 0.7862 - val_loss: 2.0403 - val_accuracy: 0.4603\nEpoch 51/100\n718/718 [==============================] - 150s 209ms/step - loss: 0.5958 - accuracy: 0.7899 - val_loss: 1.9742 - val_accuracy: 0.4744\nEpoch 52/100\n718/718 [==============================] - 152s 212ms/step - loss: 0.5873 - accuracy: 0.7918 - val_loss: 2.0488 - val_accuracy: 0.4695\nEpoch 53/100\n718/718 [==============================] - 151s 210ms/step - loss: 0.5606 - accuracy: 0.8042 - val_loss: 2.0977 - val_accuracy: 0.4634\nEpoch 54/100\n718/718 [==============================] - 156s 218ms/step - loss: 0.5603 - accuracy: 0.8036 - val_loss: 2.0308 - val_accuracy: 0.4713\nEpoch 55/100\n718/718 [==============================] - 142s 197ms/step - loss: 0.5430 - accuracy: 0.8098 - val_loss: 2.1223 - val_accuracy: 0.4653\nEpoch 56/100\n718/718 [==============================] - 135s 188ms/step - loss: 0.5409 - accuracy: 0.8104 - val_loss: 2.0516 - val_accuracy: 0.4664\nEpoch 57/100\n718/718 [==============================] - 146s 203ms/step - loss: 0.5364 - accuracy: 0.8102 - val_loss: 2.1573 - val_accuracy: 0.4697\nEpoch 58/100\n718/718 [==============================] - 128s 178ms/step - loss: 0.5150 - accuracy: 0.8227 - val_loss: 2.1822 - val_accuracy: 0.4659\nEpoch 59/100\n718/718 [==============================] - 128s 179ms/step - loss: 0.5107 - accuracy: 0.8230 - val_loss: 2.1402 - val_accuracy: 0.4749\nEpoch 60/100\n718/718 [==============================] - 141s 196ms/step - loss: 0.5141 - accuracy: 0.8197 - val_loss: 2.1938 - val_accuracy: 0.4674\nEpoch 61/100\n718/718 [==============================] - 128s 179ms/step - loss: 0.4935 - accuracy: 0.8285 - val_loss: 2.2862 - val_accuracy: 0.4693\nEpoch 62/100\n718/718 [==============================] - 136s 190ms/step - loss: 0.4891 - accuracy: 0.8289 - val_loss: 2.1760 - val_accuracy: 0.4681\nEpoch 63/100\n718/718 [==============================] - 156s 217ms/step - loss: 0.4755 - accuracy: 0.8369 - val_loss: 2.2834 - val_accuracy: 0.4624\nEpoch 64/100\n718/718 [==============================] - 136s 190ms/step - loss: 0.4712 - accuracy: 0.8364 - val_loss: 2.2633 - val_accuracy: 0.4742\nEpoch 65/100\n718/718 [==============================] - 138s 193ms/step - loss: 0.4517 - accuracy: 0.8462 - val_loss: 2.2563 - val_accuracy: 0.4739\nEpoch 67/100\n718/718 [==============================] - 137s 191ms/step - loss: 0.4412 - accuracy: 0.8487 - val_loss: 2.3193 - val_accuracy: 0.4711\nEpoch 68/100\n718/718 [==============================] - 132s 184ms/step - loss: 0.4443 - accuracy: 0.8470 - val_loss: 2.2647 - val_accuracy: 0.4763\nEpoch 69/100\n718/718 [==============================] - 140s 195ms/step - loss: 0.4563 - accuracy: 0.8457 - val_loss: 2.3536 - val_accuracy: 0.4706\nEpoch 70/100\n718/718 [==============================] - 128s 178ms/step - loss: 0.4361 - accuracy: 0.8474 - val_loss: 2.3123 - val_accuracy: 0.4763\nEpoch 71/100\n718/718 [==============================] - 126s 176ms/step - loss: 0.4276 - accuracy: 0.8530 - val_loss: 2.3856 - val_accuracy: 0.4613\nEpoch 72/100\n718/718 [==============================] - 126s 175ms/step - loss: 0.4318 - accuracy: 0.8504 - val_loss: 2.3655 - val_accuracy: 0.4810\nEpoch 73/100\n718/718 [==============================] - 125s 174ms/step - loss: 0.4213 - accuracy: 0.8561 - val_loss: 2.3436 - val_accuracy: 0.4664\nEpoch 74/100\n718/718 [==============================] - 126s 175ms/step - loss: 0.4081 - accuracy: 0.8601 - val_loss: 2.3566 - val_accuracy: 0.4587\nEpoch 75/100\n718/718 [==============================] - 125s 174ms/step - loss: 0.4005 - accuracy: 0.8644 - val_loss: 2.3996 - val_accuracy: 0.4775\nEpoch 76/100\n718/718 [==============================] - 126s 175ms/step - loss: 0.3953 - accuracy: 0.8678 - val_loss: 2.4122 - val_accuracy: 0.4704\nEpoch 77/100\n718/718 [==============================] - 125s 175ms/step - loss: 0.4009 - accuracy: 0.8636 - val_loss: 2.3338 - val_accuracy: 0.4800\nEpoch 78/100\n718/718 [==============================] - 136s 189ms/step - loss: 0.3882 - accuracy: 0.8681 - val_loss: 2.4797 - val_accuracy: 0.4741\nEpoch 79/100\n718/718 [==============================] - 128s 178ms/step - loss: 0.3940 - accuracy: 0.8686 - val_loss: 2.4153 - val_accuracy: 0.4777\nEpoch 80/100\n718/718 [==============================] - 128s 178ms/step - loss: 0.3612 - accuracy: 0.8780 - val_loss: 2.4352 - val_accuracy: 0.4702\nEpoch 81/100\n718/718 [==============================] - 126s 175ms/step - loss: 0.3909 - accuracy: 0.8696 - val_loss: 2.4127 - val_accuracy: 0.4730\nEpoch 82/100\n718/718 [==============================] - 125s 175ms/step - loss: 0.3823 - accuracy: 0.8707 - val_loss: 2.4624 - val_accuracy: 0.4730\nEpoch 83/100\n718/718 [==============================] - 124s 173ms/step - loss: 0.3797 - accuracy: 0.8713 - val_loss: 2.4241 - val_accuracy: 0.4796\nEpoch 84/100\n718/718 [==============================] - 124s 173ms/step - loss: 0.3517 - accuracy: 0.8818 - val_loss: 2.4440 - val_accuracy: 0.4772\nEpoch 85/100\n718/718 [==============================] - 125s 173ms/step - loss: 0.3592 - accuracy: 0.8783 - val_loss: 2.5420 - val_accuracy: 0.4810\nEpoch 86/100\n718/718 [==============================] - 127s 176ms/step - loss: 0.3574 - accuracy: 0.8811 - val_loss: 2.4871 - val_accuracy: 0.4699\nEpoch 87/100\n718/718 [==============================] - 126s 176ms/step - loss: 0.3832 - accuracy: 0.8707 - val_loss: 2.3611 - val_accuracy: 0.4770\nEpoch 88/100\n718/718 [==============================] - 124s 173ms/step - loss: 0.3487 - accuracy: 0.8846 - val_loss: 2.5185 - val_accuracy: 0.4666\nEpoch 89/100\n718/718 [==============================] - 125s 175ms/step - loss: 0.3400 - accuracy: 0.8883 - val_loss: 2.5190 - val_accuracy: 0.4770\nEpoch 90/100\n718/718 [==============================] - 125s 173ms/step - loss: 0.3308 - accuracy: 0.8886 - val_loss: 2.5412 - val_accuracy: 0.4737\nEpoch 91/100\n718/718 [==============================] - 126s 175ms/step - loss: 0.3544 - accuracy: 0.8822 - val_loss: 2.5496 - val_accuracy: 0.4716\nEpoch 92/100\n718/718 [==============================] - 126s 176ms/step - loss: 0.3408 - accuracy: 0.8851 - val_loss: 2.5577 - val_accuracy: 0.4713\nEpoch 93/100\n718/718 [==============================] - 136s 190ms/step - loss: 0.3292 - accuracy: 0.8880 - val_loss: 2.6475 - val_accuracy: 0.4770\nEpoch 94/100\n718/718 [==============================] - 126s 175ms/step - loss: 0.3115 - accuracy: 0.8964 - val_loss: 2.5920 - val_accuracy: 0.4732\nEpoch 95/100\n718/718 [==============================] - 125s 174ms/step - loss: 0.3347 - accuracy: 0.8888 - val_loss: 2.5615 - val_accuracy: 0.4699\nEpoch 96/100\n718/718 [==============================] - 125s 174ms/step - loss: 0.3302 - accuracy: 0.8907 - val_loss: 2.5839 - val_accuracy: 0.4716\nEpoch 97/100\n718/718 [==============================] - 126s 176ms/step - loss: 0.3208 - accuracy: 0.8921 - val_loss: 2.5637 - val_accuracy: 0.4655\nEpoch 98/100\n718/718 [==============================] - 136s 190ms/step - loss: 0.3064 - accuracy: 0.8989 - val_loss: 2.5935 - val_accuracy: 0.4714\nEpoch 99/100\n718/718 [==============================] - 125s 174ms/step - loss: 0.3081 - accuracy: 0.8965 - val_loss: 2.6597 - val_accuracy: 0.4697\nEpoch 100/100\n718/718 [==============================] - 126s 175ms/step - loss: 0.3003 - accuracy: 0.9009 - val_loss: 2.6825 - val_accuracy: 0.4727\n","output_type":"stream"}]}]}