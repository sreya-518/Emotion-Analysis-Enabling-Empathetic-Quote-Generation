{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac980b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "C:\\Users\\chand\\anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected emotion: sad\n",
      "Generated Quote: Generate a quote for someone feeling sad.\n",
      "\n",
      "\"I'm not going to be a sad person. I'm going be happy. And I think that's what I want to do. That's why I've been doing this for so long\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def generate_quote_with_gpt2(prompt, model, tokenizer, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer for TensorFlow\n",
    "model_name = \"gpt2\"  # You can experiment with other models like \"gpt2-medium-tf\", \"gpt2-large-tf\", etc.\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Replace detected_emotion with the actual detected emotion\n",
    "detected_emotion = \"sad\"\n",
    "prompt = f\"Generate a quote for someone feeling {detected_emotion}.\"\n",
    "\n",
    "# Generate a quote using GPT-2\n",
    "generated_quote = generate_quote_with_gpt2(prompt, model, tokenizer)\n",
    "print(f\"Detected emotion: {detected_emotion}\\nGenerated Quote: {generated_quote}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "890f3c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected emotion: surprise\n",
      "Generated Quote: Generate a quote for someone feeling surprise.\n",
      "\n",
      "\"I'm not sure if it's a good idea to use a word like 'excuse me' or 'I don't know' in a sentence,\" he said. \"I think it\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def generate_quote_with_gpt2(prompt, model, tokenizer, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer for TensorFlow\n",
    "model_name = \"gpt2\"  # You can experiment with other models like \"gpt2-medium-tf\", \"gpt2-large-tf\", etc.\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Replace detected_emotion with the actual detected emotion\n",
    "detected_emotion = \"surprise\"\n",
    "prompt = f\"Generate a quote for someone feeling {detected_emotion}.\"\n",
    "\n",
    "# Generate a quote using GPT-2\n",
    "generated_quote = generate_quote_with_gpt2(prompt, model, tokenizer)\n",
    "print(f\"Detected emotion: {detected_emotion}\\nGenerated Quote: {generated_quote}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe749c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Quote  Cat\n",
      "0  Holding on to anger is like grasping a hot coa...    1\n",
      "1  Anger is an acid that can do more harm to the ...    1\n",
      "2  Anger is an acid that can do more harm to the ...    1\n",
      "3  While seeking revenge, dig two graves - one fo...    1\n",
      "4  Anybody can become angry - that is easy, but t...    1\n",
      "Quote: Holding on to anger is like grasping a hot coal with the intent of throwing it at someone else; you are the one who gets burned.\t Category: 1\n",
      "Quote: Anger is an acid that can do more harm to the vessel in which it is stored than to anything on which it is poured.\n",
      "\t Category: 1\n",
      "Quote: Anger is an acid that can do more harm to the vessel in which it is stored than to anything on which it is poured.\n",
      "\t Category: 1\n",
      "Quote: While seeking revenge, dig two graves - one for yourself.\n",
      "\t Category: 1\n",
      "Quote: Anybody can become angry - that is easy, but to be angry with the right person and to the right degree and at the right time and for the right purpose, and in the right way - that is not within everybody's power and is not easy.\n",
      "\t Category: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from Excel file\n",
    "file_path = r\"C:\\Users\\chand\\Desktop\\New folder\\FinalYearProject\\QuotesExcel.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Check the structure of your DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Now you can access the 'quote' and 'cat' columns for further processing\n",
    "quotes = df['Quote']\n",
    "categories = df['Cat']\n",
    "\n",
    "# Example: Print the first 5 quotes and their corresponding categories\n",
    "for i in range(5):\n",
    "    print(f\"Quote: {quotes[i]}\\t Category: {categories[i]}\")\n",
    "\n",
    "# Now, you can use the quotes and categories for your emotion detection and quote generation project.\n",
    "# You may need to train a model, perform analysis, or any other task depending on your project requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40fbe830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\chand\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Training Loss: 1.5747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Average Training Loss: 1.2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Average Training Loss: 1.2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-finetuned\\\\tokenizer_config.json',\n",
       " './bert-finetuned\\\\special_tokens_map.json',\n",
       " './bert-finetuned\\\\vocab.txt',\n",
       " './bert-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from Excel\n",
    "file_path = r\"C:\\Users\\chand\\Desktop\\New folder\\FinalYearProject\\QuotesExcel.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Map category numbers to corresponding emotions\n",
    "category_mapping = {\n",
    "    0: \"angry\",\n",
    "    1: \"disgusted\",\n",
    "    2: \"fearful\",\n",
    "    3: \"happy\",\n",
    "    4: \"neutral\",\n",
    "    5: \"sad\",\n",
    "    6: \"surprised\"\n",
    "}\n",
    "\n",
    "# Combine quotes from the specified emotion category into a single text\n",
    "emotion_category = 1  # Change this number to choose a different emotion category\n",
    "quotes_in_category = df[df['Cat'] == emotion_category]['Quote'].tolist()\n",
    "labels = [emotion_category] * len(quotes_in_category)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(quotes_in_category, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Define the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(category_mapping))\n",
    "\n",
    "# Set up training parameters\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "batch_size = 8\n",
    "\n",
    "# Set device (cuda if available, otherwise cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_encodings.to(device)\n",
    "train_labels.to(device)\n",
    "val_encodings.to(device)\n",
    "val_labels.to(device)\n",
    "\n",
    "# Create PyTorch DataLoader for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids, attention_mask, labels = val_batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(val_labels.cpu().numpy(), all_preds)\n",
    "    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./bert-finetuned\")\n",
    "tokenizer.save_pretrained(\"./bert-finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
